{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from varyingsim.osi import OSI\n",
    "from varyingsim.cartpole import CartpoleEnv\n",
    "from varyingsim.util.buffers import TrajBuffer\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First we will train the OSI and prediction models disjointly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_traj = 100\n",
    "n_val = 10\n",
    "T = 200\n",
    "h = 16\n",
    "\n",
    "included_fovs = np.array([2,3])\n",
    "included_fovs_idxs = included_fovs + 4\n",
    "n_fov = len(included_fovs)\n",
    "\n",
    "shapes = [(2,)] * 4 + [(1,)] + [(n_fov,)] # qpos, qvel, prevqpos, prevqvel, action, cart mass, pole mass, end mass, rotation\n",
    "device = 'cuda'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obs_to_datum(obs, prev_obs, a):\n",
    "    \"\"\"\n",
    "    obs is qpos, qvel, prevqpos, prevqvel, action, cart mass, pole mass, end mass, rotation\n",
    "    \"\"\"\n",
    "    xy = obs[0:2]\n",
    "    xy_vel = obs[2:4]\n",
    "    xy_prev = prev_obs[0:2]\n",
    "    xy_vel_prev = prev_obs[2:4]\n",
    "#     fovs = prev_obs[included_fovs_idxs] # TODO: should this be previous?\n",
    "    fovs = obs[included_fovs_idxs] # TODO: should this be previous?\n",
    "    act = np.array(a)\n",
    "\n",
    "    return xy, xy_vel, xy_prev, xy_vel_prev, act, fovs \n",
    "\n",
    "def get_data(env, buffer, T):\n",
    "    obs = env.reset()\n",
    "    t = 0\n",
    "    \n",
    "    buffer.set_new_traj()\n",
    "    while t < T:\n",
    "        a = [np.random.rand() * 2 - 1] # we should do someting more sophisticated\n",
    "        prev_obs = obs\n",
    "        obs, rew, done, info = env.step(a)\n",
    "        datum = obs_to_datum(obs, prev_obs, a)\n",
    "        buffer.add_datum(datum)\n",
    "        t += 1\n",
    "\n",
    "def buffer_to_osi_torch(batch):\n",
    "    \"\"\"\n",
    "        takes in a batch from a TrajBuffer and returns a pytorch batch\n",
    "        of size N x h x d_in\n",
    "    \"\"\"\n",
    "    N, h, _ = batch[0].shape\n",
    "\n",
    "    x = np.concatenate([batch[0], batch[1]], axis=-1) # history of xy, xy_vel\n",
    "    y = batch[-1][:, -1] # latest fov\n",
    "\n",
    "    x_torch = torch.from_numpy(x).float()\n",
    "    y_torch = torch.from_numpy(y).float()\n",
    "\n",
    "    return x_torch, y_torch\n",
    "\n",
    "def set_end_mass_sin(env, t, scale=45):\n",
    "    env.set_end_mass(np.sin(t / scale) * 0.75 + 1.25)\n",
    "\n",
    "def set_mass_start(env, t):\n",
    "    if t == 0:\n",
    "        env.set_end_mass(np.random.uniform(0.5, 2.0))\n",
    "\n",
    "def set_rotation_start(env, t, scale=100):\n",
    "    if t == 0:\n",
    "        env.set_rotation(np.random.uniform(-0.2, 0.2))\n",
    "        \n",
    "def set_rotation_linear(env, t, scale=200):\n",
    "    if t == 0:\n",
    "        env.set_rotation(np.random.uniform(-0.2, -0.2))\n",
    "    else:\n",
    "        env.set_rotation(env.get_rotation() + 1 / scale)\n",
    "\n",
    "def set_mass_rot_1(env, t):\n",
    "    set_rotation_linear(env, t)\n",
    "    set_end_mass_sin(env, t)\n",
    "\n",
    "def visual_eval(buffer, model, h, device='cuda'):\n",
    "    val_batch = val_buffer.get_traj_batch(4, h)\n",
    "    x, y = buffer_to_osi_torch(val_batch)\n",
    "    x = x.to(device)\n",
    "    y = y.to(device)\n",
    "    y_hat = model(x) \n",
    "    print(y_hat)\n",
    "    print(y)\n",
    "\n",
    "def eval(buffer, model, h, device='cuda'):\n",
    "    # TODO: should have a method that just returns all trajectories concated\n",
    "    val_batch = val_buffer.get_traj_batch(len(buffer), h)\n",
    "    x, y = buffer_to_osi_torch(val_batch)\n",
    "    x = x.to(device)\n",
    "    y = y.to(device)\n",
    "    y_hat = model(x) \n",
    "    loss = F.mse_loss(y_hat, y)\n",
    "    return loss.item()\n",
    "\n",
    "def train_osi(env, train_buffer, val_buffer, h):\n",
    "    d_in = 2 * env.model.nq # qpos, qvel\n",
    "    d_param = len(included_fovs)\n",
    "    d_hidden_shared = 128\n",
    "    d_hidden_osi = 512\n",
    "\n",
    "    model = OSI(h, d_in, d_param, d_hidden_shared, d_hidden_osi)\n",
    "    \n",
    "    lr = 5e-4\n",
    "    batch_size = 64\n",
    "    n_iters = 20000\n",
    "    print_iter = 100\n",
    "\n",
    "    model = model.to(device)\n",
    "\n",
    "    optim = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    for i in range(n_iters):\n",
    "        batch = train_buffer.get_traj_batch(batch_size, h)\n",
    "        x, y = buffer_to_osi_torch(batch)\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        \n",
    "        optim.zero_grad()\n",
    "        y_hat = model(x)\n",
    "\n",
    "        loss = F.mse_loss(y_hat, y)\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "\n",
    "        if i % print_iter == 0:\n",
    "            val_batch = val_buffer.get_traj_batch(128, h)\n",
    "            x_val, y_val = buffer_to_osi_torch(val_batch)\n",
    "            x_val, y_val = x_val.to(device), y_val.to(device)\n",
    "            y_hat_val = model(x_val)\n",
    "            val_loss = F.mse_loss(y_hat_val, y_val)\n",
    "            print(i, loss.item(), val_loss.item())\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = CartpoleEnv(set_param_fn=set_mass_rot_1)\n",
    "\n",
    "train_buffer = TrajBuffer(-1, shapes)\n",
    "val_buffer = TrajBuffer(-1, shapes)\n",
    "\n",
    "for i in range(n_traj):\n",
    "    get_data(env, train_buffer, T)\n",
    "for i in range(n_val):\n",
    "    get_data(env, val_buffer, T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1.4224426746368408 0.9068495035171509\n",
      "100 0.05700865015387535 0.04756813868880272\n",
      "200 0.014974350109696388 0.01455683633685112\n",
      "300 0.011626580730080605 0.00843704491853714\n",
      "400 0.006855139974504709 0.006138855591416359\n",
      "500 0.005568894557654858 0.006993398070335388\n",
      "600 0.005015906877815723 0.003585845697671175\n",
      "700 0.0034746925812214613 0.004358012694865465\n",
      "800 0.007666757330298424 0.0027232700958848\n",
      "900 0.0020011779852211475 0.002499933587387204\n",
      "1000 0.0016353740356862545 0.001690647448413074\n",
      "1100 0.004206881858408451 0.002223699353635311\n",
      "1200 0.0019161939853802323 0.0023664310574531555\n",
      "1300 0.0012190708657726645 0.0025459923781454563\n",
      "1400 0.001605580560863018 0.0025848085060715675\n",
      "1500 0.0012554086279124022 0.0026614568196237087\n",
      "1600 0.0016420090105384588 0.0019062471110373735\n",
      "1700 0.0011739733163267374 0.0019583427347242832\n",
      "1800 0.0016214711358770728 0.0022418920416384935\n",
      "1900 0.0017042574472725391 0.0015813539503142238\n",
      "2000 0.0013134102337062359 0.0030885394662618637\n",
      "2100 0.000443446624558419 0.0017238408327102661\n",
      "2200 0.0011889683082699776 0.0009444074239581823\n",
      "2300 0.0011826917761936784 0.0025932868011295795\n",
      "2400 0.001030982006341219 0.0026905210688710213\n",
      "2500 0.001361933653242886 0.0025718582328408957\n",
      "2600 0.000758693553507328 0.0030907820910215378\n",
      "2700 0.001667868928052485 0.0013193122576922178\n",
      "2800 0.0007426872616633773 0.0026488187722861767\n",
      "2900 0.0011584199965000153 0.0024223080836236477\n",
      "3000 0.0009691440500319004 0.002026861999183893\n",
      "3100 0.0006701102247461677 0.002418525516986847\n",
      "3200 0.0016003209166228771 0.0017885980196297169\n",
      "3300 0.001841462217271328 0.0018841552082449198\n",
      "3400 0.0012887695338577032 0.0009052144014276564\n",
      "3500 0.0012500337325036526 0.003188299247995019\n",
      "3600 0.0007204666035249829 0.0013476645108312368\n",
      "3700 0.0016855004942044616 0.0014111872296780348\n",
      "3800 0.0015951180830597878 0.0015984715428203344\n",
      "3900 0.0010986746056005359 0.0015853819204494357\n",
      "4000 0.000986837432719767 0.001766732195392251\n",
      "4100 0.0007752989185974002 0.002179959090426564\n",
      "4200 0.0009563759667798877 0.0025126535911113024\n",
      "4300 0.0008468306041322649 0.0012837473768740892\n",
      "4400 0.00152147701010108 0.0005788959097117186\n",
      "4500 0.0015252784360200167 0.00158119504339993\n",
      "4600 0.0005454875063151121 0.0017463943222537637\n",
      "4700 0.000928285124246031 0.0019188551232218742\n",
      "4800 0.001358723733574152 0.002005636226385832\n",
      "4900 0.0015268907882273197 0.003653334453701973\n",
      "5000 0.0012286484707146883 0.0016194174531847239\n",
      "5100 0.0010867401724681258 0.0022084314841777086\n",
      "5200 0.0008809543796814978 0.0013033093418926\n",
      "5300 0.000818967295344919 0.0027024266310036182\n",
      "5400 0.0018881355645135045 0.0018706450937315822\n",
      "5500 0.0010933283483609557 0.002356130862608552\n",
      "5600 0.001053115352988243 0.0010796607239171863\n",
      "5700 0.0007382868207059801 0.0016078951302915812\n",
      "5800 0.0009127664379775524 0.001469708513468504\n",
      "5900 0.0007407652447000146 0.0009944610064849257\n",
      "6000 0.0010157962096855044 0.0026789880357682705\n",
      "6100 0.0014366823015734553 0.0016212232876569033\n",
      "6200 0.0015730704180896282 0.0018447153270244598\n",
      "6300 0.0015748597215861082 0.0027102287858724594\n",
      "6400 0.002276873216032982 0.0013980442890897393\n",
      "6500 0.0021179579198360443 0.001991662662476301\n",
      "6600 0.0011216762941330671 0.0005065836594440043\n",
      "6700 0.0006465006154030561 0.0012652062578126788\n",
      "6800 0.0010866002412512898 0.0014739716425538063\n",
      "6900 0.00036950584035366774 0.0016532381996512413\n",
      "7000 0.0009265205007977784 0.0011067313607782125\n",
      "7100 0.0015067839995026588 0.0014157895930111408\n",
      "7200 0.001174714881926775 0.0014202542370185256\n",
      "7300 0.0011869367444887757 0.001048797508701682\n",
      "7400 0.0007209749892354012 0.0021982514299452305\n",
      "7500 0.0013378339353948832 0.0012236095499247313\n",
      "7600 0.000985594466328621 0.0010681027779355645\n",
      "7700 0.0014025842538103461 0.001702278619632125\n",
      "7800 0.0021609370596706867 0.001952832331880927\n",
      "7900 0.0020248773507773876 0.001975334482267499\n",
      "8000 0.0020769217517226934 0.0021978795994073153\n",
      "8100 0.0006635577883571386 0.0013425203505903482\n",
      "8200 0.0011198187712579966 0.0014570364728569984\n",
      "8300 0.0006960027385503054 0.0009729403536766768\n",
      "8400 0.0011962936259806156 0.002950749360024929\n",
      "8500 0.0016805005725473166 0.0030927013140171766\n",
      "8600 0.0005226497887633741 0.0009872571099549532\n",
      "8700 0.0005132791120558977 0.0011128683108836412\n",
      "8800 0.0007666730089113116 0.0014519061660394073\n",
      "8900 0.002790244063362479 0.0010278185363858938\n",
      "9000 0.0006659942446276546 0.0016797995194792747\n",
      "9100 0.0011064496356993914 0.001078873174265027\n",
      "9200 0.0008783778175711632 0.0014019779628142715\n",
      "9300 0.0012021592119708657 0.0011298001045361161\n",
      "9400 0.002270585624501109 0.0022065797820687294\n",
      "9500 0.0013509420678019524 0.0012405773159116507\n",
      "9600 0.0008226657519116998 0.0013700283598154783\n",
      "9700 0.002166861668229103 0.0007861827034503222\n",
      "9800 0.0006373663200065494 0.0011139179114252329\n",
      "9900 0.0005339861381798983 0.0016715569654479623\n",
      "10000 0.0013061161153018475 0.001152639975771308\n",
      "10100 0.0004717773408629 0.0011492179473862052\n",
      "10200 0.0009275467600673437 0.0007094641332514584\n",
      "10300 0.0007267798064276576 0.0010584002593532205\n",
      "10400 0.000796452455688268 0.0011481854598969221\n",
      "10500 0.0019913495052605867 0.0018922962481155992\n",
      "10600 0.0005880061071366072 0.001644204487092793\n",
      "10700 0.0006062017055228353 0.0010163071565330029\n",
      "10800 0.0008466867730021477 0.0015101443277671933\n",
      "10900 0.0005640845047309995 0.0009163320646621287\n",
      "11000 0.0007797102443873882 0.0008813731838017702\n",
      "11100 0.0007595998467877507 0.0020114299841225147\n",
      "11200 0.0017883856780827045 0.0015877417754381895\n",
      "11300 0.001731487805955112 0.0014297561720013618\n",
      "11400 0.0009611542918719351 0.001181215513497591\n",
      "11500 0.0008791873697191477 0.0017039533704519272\n",
      "11600 0.000737310154363513 0.0011705926153808832\n",
      "11700 0.0005374675383791327 0.0010314042447134852\n",
      "11800 0.0025816340930759907 0.0029854350723326206\n",
      "11900 0.0004630177281796932 0.0007096341578289866\n",
      "12000 0.0009068133076652884 0.0010094174649566412\n",
      "12100 0.0004584339912980795 0.0012614798033609986\n",
      "12200 0.0012997337616980076 0.0015613274881616235\n",
      "12300 0.000800970708951354 0.0011836650082841516\n",
      "12400 0.0013737191911786795 0.001368849421851337\n",
      "12500 0.0006309605669230223 0.0009530874667689204\n",
      "12600 0.0010567386634647846 0.0011317277094349265\n",
      "12700 0.0013879708712920547 0.0010341242887079716\n",
      "12800 0.0009640575153753161 0.0008678489830344915\n",
      "12900 0.0006473249522969127 0.0010745989857241511\n",
      "13000 0.0011964316945523024 0.0009293652838096023\n",
      "13100 0.0005124073941260576 0.0007951681036502123\n",
      "13200 0.0004157765652053058 0.0017888285219669342\n",
      "13300 0.000597427599132061 0.0011971420608460903\n",
      "13400 0.0009331654291599989 0.0008261018083430827\n",
      "13500 0.0004057594633195549 0.0011923160636797547\n",
      "13600 0.0007361492607742548 0.0011696401052176952\n",
      "13700 0.0006016235565766692 0.0011049057357013226\n",
      "13800 0.00041214184602722526 0.001269511179998517\n",
      "13900 0.000848020426928997 0.0008414172334596515\n",
      "14000 0.0005757298204116523 0.0009115057764574885\n",
      "14100 0.000545097456779331 0.0014501371188089252\n",
      "14200 0.0008740980410948396 0.0010582698741927743\n",
      "14300 0.0011112854117527604 0.0015261771623045206\n",
      "14400 0.0008031321922317147 0.0016033458523452282\n",
      "14500 0.0006539086461998522 0.0009573969291523099\n",
      "14600 0.0008131314534693956 0.0011764138471335173\n",
      "14700 0.0004513622261583805 0.0012169028632342815\n",
      "14800 0.0007080473005771637 0.0014278872404247522\n",
      "14900 0.0009610690176486969 0.0009258931968361139\n",
      "15000 0.0005942711140960455 0.0014119641855359077\n",
      "15100 0.00047736853593960404 0.0005093230865895748\n",
      "15200 0.002338061109185219 0.001656124135479331\n",
      "15300 0.0007231158670037985 0.0010068751871585846\n",
      "15400 0.0020729764364659786 0.0015999929746612906\n",
      "15500 0.00093579210806638 0.0015208841068670154\n",
      "15600 0.0003509339294396341 0.0008209519437514246\n",
      "15700 0.0007449770346283913 0.0013843735214322805\n",
      "15800 0.0015426169848069549 0.000700686126947403\n",
      "15900 0.0005183448665775359 0.001277576433494687\n",
      "16000 0.00030845365836285055 0.0010148254223167896\n",
      "16100 0.0006362283020280302 0.000667488609906286\n",
      "16200 0.00042282682261429727 0.0011575478129088879\n",
      "16300 0.0006987235974520445 0.0015118045266717672\n",
      "16400 0.0006091524264775217 0.0008128360495902598\n",
      "16500 0.0005448462907224894 0.0026157372631132603\n",
      "16600 0.0004201485717203468 0.00132386339828372\n",
      "16700 0.0007506742840632796 0.0008194544934667647\n",
      "16800 0.0006339489482343197 0.0009940757881850004\n",
      "16900 0.00042920850683003664 0.0007750850054435432\n",
      "17000 0.0009079279843717813 0.0012189311673864722\n",
      "17100 0.0006586935487575829 0.0010315959807485342\n",
      "17200 0.000578335952013731 0.0026940149255096912\n",
      "17300 0.0005361509975045919 0.0010209113825112581\n",
      "17400 0.0008101751445792615 0.0005105152959004045\n",
      "17500 0.0007297582342289388 0.0010508970590308309\n",
      "17600 0.000623311207164079 0.000633311690762639\n",
      "17700 0.0002108247426804155 0.0013321993174031377\n",
      "17800 0.0006626955000683665 0.0010397114092484117\n",
      "17900 0.0003197606420144439 0.0011515861842781305\n",
      "18000 0.00032711069798097014 0.0008493069326505065\n",
      "18100 0.00042748876148834825 0.0005467069568112493\n",
      "18200 0.000631159171462059 0.0010430008405819535\n",
      "18300 0.0003621123905759305 0.0009827581234276295\n",
      "18400 0.0003348262398503721 0.001097115222364664\n",
      "18500 0.00027956702979281545 0.000625403830781579\n",
      "18600 0.00031346792820841074 0.001061182003468275\n",
      "18700 0.0007218888495117426 0.0011321844067424536\n",
      "18800 0.0005512075731530786 0.0009580149780958891\n",
      "18900 0.0009334334172308445 0.001298221293836832\n",
      "19000 0.000283435161691159 0.001047452911734581\n",
      "19100 0.0005341824144124985 0.0009113721316680312\n",
      "19200 0.00042326486436650157 0.0006182243814691901\n",
      "19300 0.0005341089563444257 0.0009006858454085886\n",
      "19400 0.000585635774768889 0.0009255015756934881\n",
      "19500 0.0004630282346624881 0.0004003971698693931\n",
      "19600 0.000620985054410994 0.0013309556525200605\n",
      "19700 0.0003285523853264749 0.0012163780629634857\n",
      "19800 0.00024429993936792016 0.000604613684117794\n",
      "19900 0.0004051712458021939 0.0008796572219580412\n"
     ]
    }
   ],
   "source": [
    "osi_model = train_osi(env, train_buffer, val_buffer, h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0009970735991373658\n",
      "tensor([[1.9163, 0.2647],\n",
      "        [1.8818, 0.2865],\n",
      "        [1.9883, 0.1786],\n",
      "        [1.3256, 0.4889]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "tensor([[1.9097, 0.2650],\n",
      "        [1.8754, 0.2850],\n",
      "        [1.9872, 0.1950],\n",
      "        [1.4375, 0.4500]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "eval_loss = eval(val_buffer, osi_model, h)\n",
    "print(eval_loss)\n",
    "visual_eval(val_buffer, osi_model, h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.10729672  0.99422704]\n",
      "[ 0.16899609 -0.98561672]\n"
     ]
    }
   ],
   "source": [
    "theta1, theta2 = 1.6783, 4.8822\n",
    "R1 = np.array([[np.cos(theta1), -np.sin(theta1)], [np.sin(theta1), np.cos(theta1)]]) \n",
    "R2 = np.array([[np.cos(theta2), -np.sin(theta2)], [np.sin(theta2), np.cos(theta2)]])\n",
    "\n",
    "x = np.array([1, 0.])\n",
    "\n",
    "print(R1 @ x)\n",
    "print(R2 @ x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean y mse 0.08356875000000011\n"
     ]
    }
   ],
   "source": [
    "batch = val_buffer.get_batch(16)\n",
    "fov = batch[-1]\n",
    "ys = fov[:, 1]\n",
    "y_mean = np.mean(ys)\n",
    "print('mean y mse', np.mean((ys - y_mean)**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(198,)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = val_buffer.get_batch(1000)\n",
    "fov = batch[-1]\n",
    "ys = fov[:, 1]\n",
    "np.unique(ys).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def traj_buffer_to_pred_torch(batch):\n",
    "    # qpos, qvel, prevqpos, prevqvel, action, friction\n",
    "    qpos = batch[0]\n",
    "    qvel = batch[1]\n",
    "    prev_qpos = batch[2]\n",
    "    prev_qvel = batch[3]\n",
    "    act = batch[4]\n",
    "    fov = batch[5]\n",
    "    x = np.concatenate([prev_qpos[:, -1], prev_qvel[:, -1], act[:, -1]], axis=-1)\n",
    "    y = np.concatenate([qpos[:, -1], qvel[:, -1]], axis=-1)\n",
    "    x_prev = np.concatenate([prev_qpos[:, -1], prev_qvel[:, -1]], axis=-1)\n",
    "    x_torch = torch.from_numpy(x).float()\n",
    "    y_torch = torch.from_numpy(y).float()\n",
    "    x_prev_torch = torch.from_numpy(x_prev).float()\n",
    "    fov_torch = torch.from_numpy(fov[:, -1]).float()\n",
    "    return x_torch, y_torch, x_prev_torch, fov_torch\n",
    "\n",
    "def batch_buffer_to_pred_torch(batch):\n",
    "    qpos = batch[0]\n",
    "    qvel = batch[1]\n",
    "    prev_qpos = batch[2]\n",
    "    prev_qvel = batch[3]\n",
    "    act = batch[4]\n",
    "    fov = batch[5]\n",
    "    x = np.concatenate([prev_qpos, prev_qvel, act], axis=-1)\n",
    "    y = np.concatenate([qpos, qvel], axis=-1)\n",
    "    x_prev = np.concatenate([prev_qpos, prev_qvel], axis=-1)\n",
    "    x_torch = torch.from_numpy(x).float()\n",
    "    y_torch = torch.from_numpy(y).float()\n",
    "    x_prev_torch = torch.from_numpy(x_prev).float()\n",
    "    fov_torch = torch.from_numpy(fov).float()\n",
    "    return x_torch, y_torch, x_prev_torch, fov_torch\n",
    "\n",
    "def train_pred_model(train_buffer, val_buffer, include_fov=True):\n",
    "    d_in = 5 + include_fov * n_fov\n",
    "    d_out = 4 # qpos, qvel\n",
    "    d_hidden = 256\n",
    "    \n",
    "    model = nn.Sequential(\n",
    "        nn.Linear(d_in, d_hidden),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(d_hidden, d_hidden),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(d_hidden, d_hidden),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(d_hidden, d_out)\n",
    "    )\n",
    "    \n",
    "    lr = 1e-3\n",
    "    batch_size = 64\n",
    "    n_iters = 6000\n",
    "    print_iter = 250\n",
    "    device = 'cuda'\n",
    "    \n",
    "    model = model.to(device)\n",
    "    \n",
    "    optim = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    \n",
    "    for i in range(n_iters):\n",
    "        batch = train_buffer.get_batch(batch_size)\n",
    "        x, y, x_prev, fov = batch_buffer_to_pred_torch(batch)\n",
    "        if include_fov:\n",
    "            x = torch.cat([x, fov], dim=1)\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        x_prev = x_prev.to(device)\n",
    "        optim.zero_grad()\n",
    "        \n",
    "        y_hat = x_prev + model(x)\n",
    "\n",
    "        loss = F.mse_loss(y_hat, y)\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "\n",
    "        if i % print_iter == 0:\n",
    "            val_batch = val_buffer.get_batch(128)\n",
    "            x_val, y_val, x_prev_val, fov_val = batch_buffer_to_pred_torch(val_batch)\n",
    "            if include_fov:\n",
    "                x_val = torch.cat([x_val, fov_val], dim=1)\n",
    "            x_val, y_val = x_val.to(device), y_val.to(device)\n",
    "            x_prev_val = x_prev_val.to(device)\n",
    "            y_hat_val = x_prev_val + model(x_val)\n",
    "            val_loss = F.mse_loss(y_hat_val, y_val)\n",
    "            print(i, loss.item(), val_loss.item())\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.0052911750972270966 0.0011716753942891955\n",
      "250 2.1536550320888637e-06 1.1934016583836637e-06\n",
      "500 5.283370683173416e-06 4.259180059307255e-06\n",
      "750 3.8025586945877876e-06 1.5275388705049409e-06\n",
      "1000 7.819514848961262e-07 2.6891791549132904e-06\n",
      "1250 3.1972626857168507e-06 1.0309903700544965e-05\n",
      "1500 9.213619023284991e-07 8.596080078859814e-07\n",
      "1750 6.6640104705584235e-06 7.755462320346851e-06\n",
      "2000 3.1569811653753277e-06 2.3217730813485105e-06\n",
      "2250 1.34729680212331e-06 1.2436260021786438e-06\n",
      "2500 7.120922873582458e-07 1.0594363857308053e-06\n",
      "2750 1.144125008067931e-06 1.4633901628258172e-06\n",
      "3000 4.6179354740161216e-07 4.5199689679975563e-07\n",
      "3250 3.2549286288485746e-07 3.081179329456063e-07\n",
      "3500 1.1396864465496037e-06 9.823902473726775e-07\n",
      "3750 2.696561693937838e-07 2.657852746779099e-07\n",
      "4000 3.5788278296422504e-07 3.659079084172845e-07\n",
      "4250 3.3056585380109027e-07 2.5176666440529516e-07\n",
      "4500 6.02884085765254e-07 7.893403335401672e-07\n",
      "4750 3.133432642243861e-07 9.785650263438583e-07\n",
      "5000 9.908123956847703e-07 1.7717926539262407e-06\n",
      "5250 1.1465149327705149e-06 8.243059710366651e-07\n",
      "5500 1.4497743450192502e-06 4.7376324801007286e-06\n",
      "5750 4.6469909875668236e-07 4.1276490492236917e-07\n",
      "0 0.0033644852228462696 0.0005042367847636342\n",
      "250 3.214836397091858e-05 2.419650809315499e-05\n",
      "500 1.792586590454448e-05 1.2856431567342952e-05\n",
      "750 1.8546703358879313e-05 1.909812090161722e-05\n",
      "1000 1.859396434156224e-05 7.777416612952948e-06\n",
      "1250 2.6067646103911102e-05 1.8262317098560743e-05\n",
      "1500 2.1856512830709107e-05 9.018453056341968e-06\n",
      "1750 1.963045542652253e-05 1.308790706389118e-05\n",
      "2000 8.4599769252236e-06 1.1976119822065812e-05\n",
      "2250 7.251280749187572e-06 1.3944452803116292e-05\n",
      "2500 3.6412207009561826e-06 1.677716863923706e-05\n",
      "2750 9.459552529733628e-06 1.587559745530598e-05\n",
      "3000 2.2157617422635667e-05 2.7084177418146282e-05\n",
      "3250 1.4861660019960254e-05 1.4757645658391993e-05\n",
      "3500 1.7463542462792248e-05 1.0725188076321501e-05\n",
      "3750 4.567998621496372e-05 4.790234015672468e-06\n",
      "4000 5.50935510545969e-05 8.601356057624798e-06\n",
      "4250 3.978274889959721e-06 7.24052733858116e-06\n",
      "4500 2.010384741879534e-05 9.787433555175085e-06\n",
      "4750 2.3537442757515237e-05 7.224900400615297e-06\n",
      "5000 1.1839847502415068e-05 8.182741112250369e-06\n",
      "5250 1.0691272109397687e-05 6.426405434467597e-06\n",
      "5500 7.457361061824486e-05 1.5418419934576377e-05\n",
      "5750 4.303854439058341e-06 1.0902198482654057e-05\n"
     ]
    }
   ],
   "source": [
    "pred_model_gt = train_pred_model(train_buffer, val_buffer)\n",
    "pred_model_no_fov = train_pred_model(train_buffer, val_buffer, include_fov=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_osi_pred(osi_model, pred_model, buffer, h, val_size=5000, calc_fov=True):\n",
    "    batch = buffer.get_traj_batch(val_size, h)\n",
    "    x_hist, gt_fov = buffer_to_osi_torch(batch)\n",
    "    x, y, x_prev, gt_fov = traj_buffer_to_pred_torch(batch)\n",
    "    \n",
    "    x_hist = x_hist.to(device)\n",
    "    gt_fov = gt_fov.to(device)\n",
    "    x = x.to(device)\n",
    "    x_prev = x_prev.to(device)\n",
    "    y = y.to(device)\n",
    "    \n",
    "    estimated_fov = osi_model(x_hist)\n",
    "    x = torch.cat([x, estimated_fov], dim=1)\n",
    "    \n",
    "    delta =  pred_model(x)\n",
    "\n",
    "    y_hat = x_prev + delta\n",
    "    \n",
    "    pos_mse = F.mse_loss(y_hat, y)\n",
    "    if calc_fov:\n",
    "        fov_mse = F.mse_loss(estimated_fov, gt_fov)\n",
    "        return pos_mse.item(), fov_mse.item()\n",
    "    else:\n",
    "        return pos_mse.item()\n",
    "\n",
    "def eval_pred_gt(pred_model, buffer, val_size=5000, include_fov=True):\n",
    "    batch = buffer.get_batch(val_size)\n",
    "    x, y, x_prev, gt_fov = batch_buffer_to_pred_torch(batch)\n",
    "    \n",
    "    gt_fov = gt_fov.to(device)\n",
    "    x = x.to(device)\n",
    "    x_prev = x_prev.to(device)\n",
    "    y = y.to(device)\n",
    "    \n",
    "    if include_fov:\n",
    "        x = torch.cat([x, gt_fov], dim=1)\n",
    "    \n",
    "    y_hat = x_prev + pred_model(x)\n",
    "    \n",
    "    pos_mse = F.mse_loss(y_hat, y)\n",
    "    return pos_mse.item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.769854626829328e-07 3.498588512229617e-07 1.2562995834741741e-05\n"
     ]
    }
   ],
   "source": [
    "pos_mse_osi, fov_mse = eval_osi_pred(osi_model, pred_model_gt, val_buffer, h)\n",
    "pos_gt_mse = eval_pred_gt(pred_model_gt, val_buffer)\n",
    "pos_no_fov_mse = eval_pred_gt(pred_model_no_fov, val_buffer, include_fov=False)\n",
    "\n",
    "print(pos_mse_osi, pos_gt_mse, pos_no_fov_mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f2b4d095100>]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(val_buffer.trajectories[3][-1][:, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now can we learn to predict without the ground truth labels?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_osi_pred(env, train_buffer, val_buffer, h, d_latent=2):\n",
    "    osi_d_in = 2 * env.model.nq # qpos, qvel\n",
    "    osi_d_param = d_latent # TODO: this could be more than 2 since we don't have the true labels\n",
    "    osi_d_hidden_shared = 128\n",
    "    osi_d_hidden_osi = 512\n",
    "\n",
    "    osi_model = OSI(h, osi_d_in, osi_d_param, osi_d_hidden_shared, osi_d_hidden_osi)\n",
    "    \n",
    "    pred_d_in = 5 +  osi_d_param\n",
    "    pred_d_out = 4 # qpos, qvel\n",
    "    pred_d_hidden = 256\n",
    "    \n",
    "    pred_model = nn.Sequential(\n",
    "        nn.Linear(pred_d_in, pred_d_hidden),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(pred_d_hidden, pred_d_hidden),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(pred_d_hidden, pred_d_hidden),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(pred_d_hidden, pred_d_out)\n",
    "    )\n",
    "    \n",
    "    lr = 5e-4\n",
    "    batch_size = 64\n",
    "    n_iters = 20000\n",
    "    print_iter = 250\n",
    "\n",
    "    osi_model = osi_model.to(device)\n",
    "    pred_model = pred_model.to(device)\n",
    "\n",
    "    optim = torch.optim.Adam(list(osi_model.parameters()) + list(pred_model.parameters()), lr=lr)\n",
    "\n",
    "    for i in range(n_iters):\n",
    "        batch = train_buffer.get_traj_batch(batch_size, h)\n",
    "        x_osi, y_osi = buffer_to_osi_torch(batch)\n",
    "        x_pred, y_pred, x_prev_pred, fov_pred = traj_buffer_to_pred_torch(batch)\n",
    "        \n",
    "        x_osi, y_osi = x_osi.to(device), y_osi.to(device)\n",
    "        x_pred, y_pred, x_prev_pred, fov_pred = x_pred.to(device), y_pred.to(device), x_prev_pred.to(device), fov_pred.to(device)\n",
    "        \n",
    "        optim.zero_grad()\n",
    "        \n",
    "        est_params = osi_model(x_osi)\n",
    "        x_in = torch.cat([x_pred, est_params], dim=1)\n",
    "        \n",
    "        delta = pred_model(x_in)\n",
    "        y_hat = x_prev_pred + delta\n",
    "\n",
    "        loss = F.mse_loss(y_hat, y_pred)\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "\n",
    "        if i % print_iter == 0:\n",
    "            val_batch = train_buffer.get_traj_batch(128, h)\n",
    "            x_osi_val, y_osi_val = buffer_to_osi_torch(batch)\n",
    "            x_pred_val, y_pred_val, x_prev_pred_val, fov_pred_val = traj_buffer_to_pred_torch(batch)\n",
    "\n",
    "            x_osi_val, y_osi_val = x_osi_val.to(device), y_osi_val.to(device)\n",
    "            x_pred_val, y_pred_val, x_prev_pred_val, fov_pred_val = x_pred_val.to(device), y_pred_val.to(device), x_prev_pred_val.to(device), fov_pred_val.to(device)\n",
    "\n",
    "            est_params_val = osi_model(x_osi_val)\n",
    "            x_in_val = torch.cat([x_pred_val, est_params_val], dim=1)\n",
    "\n",
    "            delta_val = pred_model(x_in_val)\n",
    "            y_hat_val = x_prev_pred_val + delta_val\n",
    "\n",
    "            val_loss = F.mse_loss(y_hat, y_pred)\n",
    "            print(i, loss.item(), val_loss.item())\n",
    "    return osi_model, pred_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.0026811622083187103 0.0026811622083187103\n",
      "250 1.7655938790994696e-05 1.7655938790994696e-05\n",
      "500 1.3452666962621151e-06 1.3452666962621151e-06\n",
      "750 2.1691123492928455e-06 2.1691123492928455e-06\n",
      "1000 8.768424208938086e-07 8.768424208938086e-07\n",
      "1250 8.402882940572454e-07 8.402882940572454e-07\n",
      "1500 1.4805086721025873e-06 1.4805086721025873e-06\n",
      "1750 1.4804693364567356e-06 1.4804693364567356e-06\n",
      "2000 1.0608497404973605e-06 1.0608497404973605e-06\n",
      "2250 1.4160747241476201e-06 1.4160747241476201e-06\n",
      "2500 1.2483822047215654e-06 1.2483822047215654e-06\n",
      "2750 1.2667294413404306e-06 1.2667294413404306e-06\n",
      "3000 5.587359623859811e-07 5.587359623859811e-07\n",
      "3250 3.0487339586215967e-07 3.0487339586215967e-07\n",
      "3500 7.565745363535825e-07 7.565745363535825e-07\n",
      "3750 9.618295280233724e-07 9.618295280233724e-07\n",
      "4000 3.875410925502365e-07 3.875410925502365e-07\n",
      "4250 5.921282308918308e-07 5.921282308918308e-07\n",
      "4500 1.032562181535468e-06 1.032562181535468e-06\n",
      "4750 9.755324299476342e-07 9.755324299476342e-07\n",
      "5000 5.906116484766244e-07 5.906116484766244e-07\n",
      "5250 4.2030555391647795e-07 4.2030555391647795e-07\n",
      "5500 3.899252192240965e-07 3.899252192240965e-07\n",
      "5750 9.288737601309549e-07 9.288737601309549e-07\n",
      "6000 4.4983050884184195e-07 4.4983050884184195e-07\n",
      "6250 4.4118385744695843e-07 4.4118385744695843e-07\n",
      "6500 5.190722731640562e-07 5.190722731640562e-07\n",
      "6750 1.254027779395983e-06 1.254027779395983e-06\n",
      "7000 4.638906148102251e-07 4.638906148102251e-07\n",
      "7250 2.0359019003990397e-07 2.0359019003990397e-07\n",
      "7500 4.17683111209044e-07 4.17683111209044e-07\n",
      "7750 7.651935334251903e-07 7.651935334251903e-07\n",
      "8000 3.1758855811858666e-07 3.1758855811858666e-07\n",
      "8250 4.3342708977434086e-07 4.3342708977434086e-07\n",
      "8500 1.3711530755244894e-06 1.3711530755244894e-06\n",
      "8750 2.51704449283352e-07 2.51704449283352e-07\n",
      "9000 5.842497330377228e-07 5.842497330377228e-07\n",
      "9250 5.624737013931735e-07 5.624737013931735e-07\n",
      "9500 3.585912509151967e-07 3.585912509151967e-07\n",
      "9750 7.393545047307271e-07 7.393545047307271e-07\n",
      "10000 1.717068585094239e-07 1.717068585094239e-07\n",
      "10250 4.075630499755789e-07 4.075630499755789e-07\n",
      "10500 7.70050519349752e-07 7.70050519349752e-07\n",
      "10750 3.881706902575388e-07 3.881706902575388e-07\n",
      "11000 2.770148341824097e-07 2.770148341824097e-07\n",
      "11250 4.994066102881334e-07 4.994066102881334e-07\n",
      "11500 2.819954829647031e-07 2.819954829647031e-07\n",
      "11750 3.139888633540977e-07 3.139888633540977e-07\n",
      "12000 3.7815527775819646e-07 3.7815527775819646e-07\n",
      "12250 2.9010698199272156e-07 2.9010698199272156e-07\n",
      "12500 1.1351248758728616e-06 1.1351248758728616e-06\n",
      "12750 4.0348595575778745e-07 4.0348595575778745e-07\n",
      "13000 2.1624617829729686e-07 2.1624617829729686e-07\n",
      "13250 1.5047839951876085e-07 1.5047839951876085e-07\n",
      "13500 3.6690005345008103e-07 3.6690005345008103e-07\n",
      "13750 1.940936726896325e-07 1.940936726896325e-07\n",
      "14000 6.211635081854183e-07 6.211635081854183e-07\n",
      "14250 3.9884605484985514e-07 3.9884605484985514e-07\n",
      "14500 6.17627051724412e-07 6.17627051724412e-07\n",
      "14750 2.5478027509961976e-07 2.5478027509961976e-07\n",
      "15000 3.660815082184854e-07 3.660815082184854e-07\n",
      "15250 2.6415014531266934e-07 2.6415014531266934e-07\n",
      "15500 5.22059679042286e-07 5.22059679042286e-07\n",
      "15750 3.9314517152888584e-07 3.9314517152888584e-07\n",
      "16000 2.3940981463965727e-07 2.3940981463965727e-07\n",
      "16250 2.861862640202162e-07 2.861862640202162e-07\n",
      "16500 1.9498980918797315e-07 1.9498980918797315e-07\n",
      "16750 3.0287486652014195e-07 3.0287486652014195e-07\n",
      "17000 2.5189623897858837e-07 2.5189623897858837e-07\n",
      "17250 1.4413527082979272e-07 1.4413527082979272e-07\n",
      "17500 1.0998286370522692e-07 1.0998286370522692e-07\n",
      "17750 1.1668023347510825e-07 1.1668023347510825e-07\n",
      "18000 5.728010137318051e-07 5.728010137318051e-07\n",
      "18250 2.4746861981839174e-07 2.4746861981839174e-07\n",
      "18500 3.795656482452614e-07 3.795656482452614e-07\n",
      "18750 1.8038296900613204e-07 1.8038296900613204e-07\n",
      "19000 1.4996632557995326e-07 1.4996632557995326e-07\n",
      "19250 2.1690919993488933e-07 2.1690919993488933e-07\n",
      "19500 3.3598797699596616e-07 3.3598797699596616e-07\n",
      "19750 1.2457955733680137e-07 1.2457955733680137e-07\n"
     ]
    }
   ],
   "source": [
    "d_latent = 4\n",
    "osi_model_joint, pred_model_joint = train_osi_pred(env, train_buffer, val_buffer, h, d_latent=d_latent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.5111886493032216e-07 3.499540355278441e-07 1.4050422578293364e-05 2.7178907657798845e-07\n"
     ]
    }
   ],
   "source": [
    "pos_mse_osi, fov_mse = eval_osi_pred(osi_model, pred_model_gt, val_buffer, h)\n",
    "pos_gt_mse = eval_pred_gt(pred_model_gt, val_buffer)\n",
    "pos_no_fov_mse = eval_pred_gt(pred_model_no_fov, val_buffer, include_fov=False)\n",
    "joint_mse_osi = eval_osi_pred(osi_model_joint, pred_model_joint, val_buffer, h, calc_fov=False)\n",
    "\n",
    "print(pos_mse_osi, pos_gt_mse, pos_no_fov_mse, joint_mse_osi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Can we recover the rotations from the latents?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_latents_to_fov(osi_model, train_buffer, val_buffer, h, d_latent=2):\n",
    "    d_in = d_latent\n",
    "    d_out = n_fov\n",
    "    d_hidden = 32\n",
    "    \n",
    "    lr = 5e-4\n",
    "    batch_size = 64\n",
    "    n_iters = 20000\n",
    "    print_iter = 100\n",
    "\n",
    "    model = nn.Sequential(\n",
    "        nn.Linear(d_in, d_hidden),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(d_hidden, d_hidden),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(d_hidden, d_hidden),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(d_hidden, d_out)\n",
    "    )\n",
    "    \n",
    "    model = model.to(device)\n",
    "\n",
    "    optim = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    for i in range(n_iters):\n",
    "        batch = train_buffer.get_traj_batch(batch_size, h)\n",
    "        x, y = buffer_to_osi_torch(batch)\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        \n",
    "        optim.zero_grad()\n",
    "        latent = osi_model(x)\n",
    "        \n",
    "        y_hat = model(latent)\n",
    "\n",
    "        loss = F.mse_loss(y_hat, y)\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "\n",
    "        if i % print_iter == 0:\n",
    "            val_batch = val_buffer.get_traj_batch(128, h)\n",
    "            x_val, y_val = buffer_to_osi_torch(val_batch)\n",
    "            x_val, y_val = x_val.to(device), y_val.to(device)\n",
    "            latent_val = osi_model(x_val)\n",
    "            y_hat_val = model(latent_val)\n",
    "            val_loss = F.mse_loss(y_hat_val, y_val)\n",
    "            print(i, loss.item(), val_loss.item())\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 dim 1 must match mat2 dim 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-8a251bca9242>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlatent_fov_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_latents_to_fov\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mosi_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_latent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0md_latent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-18-7bf2562bc98e>\u001b[0m in \u001b[0;36mtrain_latents_to_fov\u001b[0;34m(osi_model, train_buffer, val_buffer, h, d_latent)\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mlatent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mosi_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m         \u001b[0my_hat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlatent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmse_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_hat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    115\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1672\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1673\u001b[0m         \u001b[0;31m# fused op is marginally faster\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1674\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1675\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1676\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 dim 1 must match mat2 dim 0"
     ]
    }
   ],
   "source": [
    "latent_fov_model = train_latents_to_fov(osi_model, train_buffer, val_buffer, h, d_latent=d_latent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_latent_to_fov(osi_model, fov_model, buffer, h, val_size=5000):\n",
    "    batch = buffer.get_traj_batch(val_size, h)\n",
    "    x, y = buffer_to_osi_torch(batch)\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    latent = osi_model(x)\n",
    "    y_hat = fov_model(latent)\n",
    "    loss = F.mse_loss(y_hat, y)\n",
    "    return loss.item()\n",
    "\n",
    "def visual_eval_latent_to_fov(osi_model, fov_model, buffer, h):\n",
    "    batch = buffer.get_traj_batch(4, h)\n",
    "    x, y = buffer_to_osi_torch(batch)\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    latent = osi_model(x)\n",
    "    y_hat = fov_model(latent)\n",
    "    print(y.detach().cpu().numpy())\n",
    "    print(y_hat.detach().cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_loss = eval_latent_to_fov(osi_model, latent_fov_model, val_buffer, h)\n",
    "print(eval_loss)\n",
    "visual_eval_latent_to_fov(osi_model, latent_fov_model, val_buffer, h)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
